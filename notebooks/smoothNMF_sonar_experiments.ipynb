{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "output = loadmat(os.path.join('..','ssnmf','data','sonar_output.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_W = output['params_all']['init_W'][0,0]\n",
    "init_H = output['params_all']['init_H'][0,0]\n",
    "max_iter = output['params_all']['max_iter'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15984, 62)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['LL'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing parameters:\n",
    "\n",
    "r = 3 \n",
    "\n",
    "sm = 10000000, sp = 100\n",
    "\n",
    "steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# testing no sparsity, no smoothnes, 1 iteration\n",
    "import ssnmf\n",
    "model = ssnmf.smoothNMF(r=3, max_iter=1000, betaW=0, betaH=0, smoothness=10000000, sparsity=100)\n",
    "\n",
    "\n",
    "\n",
    "# print(LA.norm(output['LL'] - (model.W@model.H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssnmf.ssnmf import _update_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install --yes line_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f _update_H model.fit(output['LL'], W=init_W, H=init_H, init = 'custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model.fit(output['LL'], W=init_W, H=init_H, init='custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.trace((model.W.T @ model.W) @ (model.W.T @ model.W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as LA\n",
    "LA.norm(model.W @ model.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tensorly to reshape\n",
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Average across time of day columns\n",
    "tod_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_reshaped = model.W.reshape(3, 144, 37, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL_reconstructed = (model.W[:,:].reshape([-1,3])@model.H[:,:].reshape([3,-1])).reshape((3,144,37,62))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[\"LL\"][:,0].reshape(3,144,37)[0,:,:].T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bin_size = 5\n",
    "tod_num = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonar time series reconstructed with 3 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "freq = [38,120,200]\n",
    "\n",
    "fig,ax = plt.subplots(3,1,figsize=(18,7),sharex=True)\n",
    "for f in range(3):\n",
    "    ax[f].imshow(tl.unfold(LL_.transpose(0,2,3,1)[f,:,:,:],0)[:,:][::-1,:],aspect='auto')\n",
    "    ax[f].set_xticks(range(0,int(62*144/tod_num),int(144/tod_num*5)))\n",
    "    ax[f].set_xticklabels(range(0,65,5))\n",
    "    ax[f].set_yticks(range(0,37,5))\n",
    "    ax[f].set_yticklabels((np.arange(0,37,5)+1)*depth_bin_size)\n",
    "    plt.setp(ax[f].get_xticklabels(), fontsize=12)\n",
    "    plt.setp(ax[f].get_yticklabels(), fontsize=12)\n",
    "    if ax[f].is_last_row():\n",
    "        plt.xlabel('Day',fontsize=14)\n",
    "    ax[f].set_ylabel('Depth (m)',fontsize=14)\n",
    "    ax[f].set_title('Frequency = %d kHz' % freq[f], fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonar time series after PCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "freq = [38,120,200]\n",
    "\n",
    "fig,ax = plt.subplots(3,1,figsize=(18,7),sharex=True)\n",
    "for f in range(3):\n",
    "    ax[f].imshow(tl.unfold(output[\"LL\"].reshape(3,144,37,62).transpose(0,2,3,1)[f,:,:,:],0)[:,:][::-1,:],aspect='auto')\n",
    "    ax[f].set_xticks(range(0,int(62*144/tod_num),int(144/tod_num*5)))\n",
    "    ax[f].set_xticklabels(range(0,65,5))\n",
    "    ax[f].set_yticks(range(0,37,5))\n",
    "    ax[f].set_yticklabels((np.arange(0,37,5)+1)*depth_bin_size)\n",
    "    plt.setp(ax[f].get_xticklabels(), fontsize=12)\n",
    "    plt.setp(ax[f].get_yticklabels(), fontsize=12)\n",
    "    if ax[f].is_last_row():\n",
    "        plt.xlabel('Day',fontsize=14)\n",
    "    ax[f].set_ylabel('Depth (m)',fontsize=14)\n",
    "    ax[f].set_title('Frequency = %d kHz' % freq[f], fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq = [38,120,200]\n",
    "\n",
    "#fig,ax = plt.subplots(3,1,figsize=(18,7),sharex=True)\n",
    "#for f in range(3):\n",
    "#    ax[f].imshow(tl.unfold(tl.tensor(out[:,:,:]),mode=1)[::-1,:],\n",
    "#                           aspect='auto', vmin=-80, vmax=-40, cmap='jet')\n",
    "#    ax[f].set_xticks(range(0,int(62*144/tod_num),int(144/tod_num*5)))\n",
    "#    ax[f].set_xticklabels(range(0,65,5))\n",
    "#    ax[f].set_yticks(range(0,37,5))\n",
    "#    ax[f].set_yticklabels((np.arange(0,37,5)+1)*depth_bin_size)\n",
    "#    plt.setp(ax[f].get_xticklabels(), fontsize=12)\n",
    "#    plt.setp(ax[f].get_yticklabels(), fontsize=12)\n",
    "#    if ax[f].is_last_row():\n",
    "#        plt.xlabel('Day',fontsize=14)\n",
    "#    ax[f].set_ylabel('Depth (m)',fontsize=14)\n",
    "#    ax[f].set_title('Frequency = %d kHz' % freq[f], fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a function for transforming the data for plotting\n",
    "# def separate_transform_result(D,ori_data,ping_per_day_mvbs,log_opt=1):\n",
    "#    '''\n",
    "#    Separate transformed results into different frequencies and \n",
    "#    for use with `plot_cmp_data_decomp` and `plot_single_day`\n",
    "#    '''\n",
    "#    D_long = D.reshape((D.shape[0],-1,ori_data.shape[1])).swapaxes(1,2)\n",
    "#    D_sep = D_long.reshape((D_long.shape[0],D_long.shape[1],-1,ping_per_day_mvbs)).transpose((2,0,1,3))\n",
    "#    if log_opt==1:\n",
    "#        D_plot = 10*np.log10(D_sep.transpose((0,2,1,3))).reshape((D_sep.shape[0],D_sep.shape[2],-1))\n",
    "#    else:\n",
    "#        D_plot = D_sep.transpose((0,2,1,3)).reshape((D_sep.shape[0],D_sep.shape[2],-1))\n",
    "#    return D_sep,D_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_sep, L_plot = separate_transform_result(output['LL'],mvbs,ping_per_day_mvbs,log_opt = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,4.5))\n",
    "#plt.imshow(np.moveaxis(output[\"LL\"][:,:].reshape(3,144,37,62),1,2)[0,:,:,:].reshape(37,-1)[:,:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,4.5))\n",
    "#plt.imshow(np.moveaxis(output[\"LL\"][:,:].reshape(144,37,62),1,2)[0,:,:,:].reshape(37,-1)[:,:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,4.5))\n",
    "#plt.imshow(np.moveaxis(B[:,:].reshape(3,144,37,62),1,2).reshape(3,37,-1)[2,:,:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coefficients in time\n",
    "plt.plot(model.H.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ssnmf.smoothNMF(r=3, max_iter=10, betaW=0, betaH=0, smoothness=0)\n",
    "\n",
    "%prun model.fit(output['LL'], W=init_W, H=init_H, init='custom')\n",
    "\n",
    "# print(LA.norm(output['LL'] - (model.W@model.H)))\n",
    "\n",
    "# plt.plot(model.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssnmf.ssnmf import _update_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f _update_H model.fit(output['LL'], W=init_W, H=init_H, init='custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with WH outcomes from Matlab\n",
    "WH = loadmat('/Users/valentina/Downloads/WH.mat')\n",
    "print(np.max(np.abs(WH['H'] - model.H)))\n",
    "print(np.max(np.abs(WH['W'] - model.W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WH_cost = loadmat('/Users/valentina/Downloads/WH_cost.mat')\n",
    "print(np.max(np.abs(WH_cost['H'] - model.H)))\n",
    "print(np.max(np.abs(WH_cost['W'] - model.W)))\n",
    "print(np.max(np.abs(WH_cost['objective'].ravel() - np.array(model.cost))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost calculation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(WH_cost['objective'].ravel()-np.array(model.cost),'ro')\n",
    "#plt.plot(model.cost)\n",
    "plt.title('Cost Calculation Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WH_cost['objective'].shape\n",
    "np.array(model.cost).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WH_cost.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Selection & Cophenetic Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssnmf\n",
    "model = ssnmf.smoothNMF(r=3, max_iter=100, betaW=0, betaH=0, smoothness=1000000, sparsity=10)\n",
    "model.fit(output['LL'], init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # from Nimfa package\n",
    "\n",
    "    def coph_cor(self, idx=None):\n",
    "        \"\"\"\n",
    "        Compute cophenetic correlation coefficient of consensus matrix, generally obtained from multiple NMF runs. \n",
    "        \n",
    "        The cophenetic correlation coefficient is measure which indicates the dispersion of the consensus matrix and is based \n",
    "        on the average of connectivity matrices. It measures the stability of the clusters obtained from NMF. \n",
    "        It is computed as the Pearson correlation of two distance matrices: the first is the distance between samples induced by the \n",
    "        consensus matrix; the second is the distance between samples induced by the linkage used in the reordering of the consensus \n",
    "        matrix [Brunet2004]_.\n",
    "        \n",
    "        Return real number. In a perfect consensus matrix, cophenetic correlation equals 1. When the entries in consensus matrix are\n",
    "        scattered between 0 and 1, the cophenetic correlation is < 1. We observe how this coefficient changes as factorization rank \n",
    "        increases. We select the first rank, where the magnitude of the cophenetic correlation coefficient begins to fall [Brunet2004]_.\n",
    "        \n",
    "        :param idx: Used in the multiple NMF model. In factorizations following standard NMF model or nonsmooth NMF model\n",
    "                    :param:`idx` is always None.\n",
    "        :type idx: None or `str` with values 'coef' or 'coef1' (`int` value of 0 or 1, respectively) \n",
    "        \"\"\"\n",
    "        A = self.consensus(idx=idx)\n",
    "        # upper diagonal elements of consensus\n",
    "        avec = np.array([A[i, j] for i in range(A.shape[0] - 1)\n",
    "                        for j in range(i + 1, A.shape[1])])\n",
    "        # consensus entries are similarities, conversion to distances\n",
    "        Y = 1 - avec\n",
    "        Z = linkage(Y, method='average')\n",
    "        # cophenetic correlation coefficient of a hierarchical clustering\n",
    "        # defined by the linkage matrix Z and matrix Y from which Z was\n",
    "        # generated\n",
    "        return cophenet(Z, Y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import eq\n",
    "def elop(X, Y, op):\n",
    "    #try:\n",
    "    #    X[X == 0] = np.finfo(X.dtype).eps\n",
    "    #    Y[Y == 0] = np.finfo(Y.dtype).eps\n",
    "    #except ValueError:\n",
    "    #    return op(np.mat(X), np.mat(Y))\n",
    "    return(op(np.mat(X), np.mat(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def connectivity(model, X, H=None, idx=None):\n",
    "        \"\"\"\n",
    "        Compute the connectivity matrix for the samples based on their mixture coefficients. \n",
    "        \n",
    "        The connectivity matrix C is a symmetric matrix which shows the shared membership of the samples: entry C_ij is 1 iff sample i and \n",
    "        sample j belong to the same cluster, 0 otherwise. Sample assignment is determined by its largest metagene expression value. \n",
    "        \n",
    "        Return connectivity matrix.\n",
    "        \n",
    "        :param idx: Used in the multiple NMF model. In factorizations following\n",
    "            standard NMF model or nonsmooth NMF model ``idx`` is always None.\n",
    "        :type idx: None or `str` with values 'coef' or 'coef1' (`int` value of 0 or 1, respectively) \n",
    "        \"\"\"\n",
    "        #X = model.X\n",
    "        H = model.H\n",
    "        idx = np.argmax(H, axis=0)\n",
    "        #plt.imshow(H)\n",
    "        plt.plot(H.T)\n",
    "        print(idx)\n",
    "        mat1 = np.tile(idx, (X.shape[1], 1))\n",
    "        mat2 = np.tile(np.reshape(idx.T,(len(idx),1)), (1, X.shape[1]))\n",
    "        conn = elop(mat1, mat2, eq)\n",
    "        #if sp.isspmatrix(conn):\n",
    "        #    return conn.__class__(conn, dtype='d')\n",
    "        #else:\n",
    "        return np.mat(conn, dtype='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = connectivity(model, output['LL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus(n_run, X, model):\n",
    "    \"\"\"\n",
    "    Compute consensus matrix as the mean connectivity matrix across multiple runs of the factorization. It has been\n",
    "    proposed by [Brunet2004]_ to help visualize and measure the stability of the clusters obtained by NMF.\n",
    "        \n",
    "    Tracking of matrix factors across multiple runs must be enabled for computing consensus matrix. For results\n",
    "    of a single NMF run, the consensus matrix reduces to the connectivity matrix.\n",
    "        \n",
    "    :param idx: Used in the multiple NMF model. In factorizations following\n",
    "            standard NMF model or nonsmooth NMF model ``idx`` is always None.\n",
    "    :type idx: None or `str` with values 'coef' or 'coef1' (`int` value of 0 or 1, respectively) \n",
    "    \"\"\"\n",
    "        #V = self.target(idx)\n",
    "        #if self.track_factor:\n",
    "        #    if sp.isspmatrix(V):\n",
    "        #        cons = V.__class__((V.shape[1], V.shape[1]), dtype=V.dtype)\n",
    "        #    else:\n",
    "        #        cons = np.mat(np.zeros((V.shape[1], V.shape[1])))\n",
    "        #    for i in range(self.n_run):\n",
    "        #        cons += self.connectivity(\n",
    "        #            H=self.tracker.get_factor(i).H, idx=idx)\n",
    "        #    return sop(cons, self.n_run, div)\n",
    "        #else:\n",
    "        #    return self.connectivity(H=self.coef(idx), idx=idx)\n",
    "    cons = np.mat(np.zeros((X.shape[1], X.shape[1])))\n",
    "    for run in range(n_run):\n",
    "        model.fit(X, init='random')\n",
    "        cons += connectivity(model, X)\n",
    "    return(cons/n_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_matrix = consensus(5, output['LL'], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cons_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coph_cor(X, model, n_runs = 1):\n",
    "        \"\"\"\n",
    "        Compute cophenetic correlation coefficient of consensus matrix, generally obtained from multiple NMF runs. \n",
    "        \n",
    "        The cophenetic correlation coefficient is measure which indicates the dispersion of the consensus matrix and is based \n",
    "        on the average of connectivity matrices. It measures the stability of the clusters obtained from NMF. \n",
    "        It is computed as the Pearson correlation of two distance matrices: the first is the distance between samples induced by the \n",
    "        consensus matrix; the second is the distance between samples induced by the linkage used in the reordering of the consensus \n",
    "        matrix [Brunet2004]_.\n",
    "        \n",
    "        Return real number. In a perfect consensus matrix, cophenetic correlation equals 1. When the entries in consensus matrix are\n",
    "        scattered between 0 and 1, the cophenetic correlation is < 1. We observe how this coefficient changes as factorization rank \n",
    "        increases. We select the first rank, where the magnitude of the cophenetic correlation coefficient begins to fall [Brunet2004]_.\n",
    "        \n",
    "        :param idx: Used in the multiple NMF model. In factorizations following standard NMF model or nonsmooth NMF model\n",
    "                    :param:`idx` is always None.\n",
    "        :type idx: None or `str` with values 'coef' or 'coef1' (`int` value of 0 or 1, respectively) \n",
    "        \"\"\"\n",
    "        A = consensus(n_runs, X, model)\n",
    "        # upper diagonal elements of consensus\n",
    "        avec = np.array([A[i, j] for i in range(A.shape[0] - 1)\n",
    "                        for j in range(i + 1, A.shape[1])])\n",
    "        # consensus entries are similarities, conversion to distances\n",
    "        Y = 1 - avec\n",
    "        Z = linkage(Y, method='average')\n",
    "        # cophenetic correlation coefficient of a hierarchical clustering\n",
    "        # defined by the linkage matrix Z and matrix Y from which Z was\n",
    "        # generated\n",
    "        return (cophenet(Z, Y)[0], cophenet(Z,Y)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, cophenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "c, D = coph_cor(LL_norm, model, n_runs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(squareform(D))\n",
    "plt.title('Cophenetic Correlation is {}'.format(round(c,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# for 1000 iterations I get a warning and returns nans\n",
    "# most probably division by zero (not clear which run)\n",
    "# the connectivity is the same for each observation: i.e. all observations are assigned to the same component\n",
    "# so the variance becomes zero and that results in dividing by zero\n",
    "# I think I need some normalization\n",
    "# It works when smoothness and sparsity are lower and there is more variation in the coefficients and they intersect each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matlab version \n",
    "function seq = find_match_factor_seq(rho,rank)\n",
    "% Find matching component sequence based on correlation coefficients\n",
    "%\n",
    "% Inputs\n",
    "%   rho   correlation coefficients (based on H or W, H seems to give better results)\n",
    "%   rank  rank of the decomposition\n",
    "%\n",
    "% Outputs\n",
    "%   seq   matching sequence\n",
    "%\n",
    "% Wu-Jung Lee | leewujung@gmail.com\n",
    "% 2019 05 03\n",
    "\n",
    "icomp = 1;\n",
    "while icomp<=rank\n",
    "    if icomp==1\n",
    "        seq = nan(rank,1);\n",
    "    end\n",
    "    [~,max_ind] = max(rho(:));\n",
    "    [x_ind,y_ind] = ind2sub(size(rho),max_ind);\n",
    "    seq(x_ind) = y_ind;\n",
    "\n",
    "    \n",
    "    icomp = icomp+1;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match_factor_seq(rho, n_components):\n",
    "    \"\"\"\n",
    "    Find matching component sequence based on correlation coefficients\n",
    "\n",
    "    Inputs\n",
    "       rho   correlation coefficients (based on H or W, H seems to give better results)\n",
    "       rank  rank of the decomposition\n",
    "\n",
    "    Outputs\n",
    "       seq   matching sequence\n",
    "    \"\"\"\n",
    "    seq = {}\n",
    "    for c in range(n_components):\n",
    "        ind = np.argmax(rho)\n",
    "        row_ind, col_ind = np.unravel_index(ind, rho.shape)\n",
    "        seq[row_ind] = col_ind\n",
    "        rho[row_ind,:] = -2\n",
    "        rho[:,col_ind] = -2\n",
    "        \n",
    "    return (seq)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = np.corrcoef(model.H.T,model.H.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = find_match_factor_seq(rho, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize X\n",
    "output['LL']\n",
    "\n",
    "LL_norm = nan(size(LL));\n",
    "sigma_all = nan(size(LL,1),1);\n",
    "for irow = 1:size(LL,1)\n",
    "    sigma = std(LL(irow,:));\n",
    "    LL_norm(irow,:) = LL(irow,:)/sigma;\n",
    "    sigma_all(irow) = sigma;\n",
    "end\n",
    "LL = LL_norm;  % use normalized data for decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the pixel values over time\n",
    "LL_norm = (output['LL'].T/np.linalg.norm(output['LL'],axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for different ranks\n",
    "c_dict = {}\n",
    "D_dict = {}\n",
    "for r in range(2,6):\n",
    "    model.rank = r\n",
    "    c, D = coph_cor(output['LL'], model, n_runs = 10)\n",
    "    c_dict[r] = c\n",
    "    D_dict[r] = D\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(*zip(*c_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(squareform(D_dict[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
